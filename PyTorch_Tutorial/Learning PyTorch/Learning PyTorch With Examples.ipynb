{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22efca85",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## LEARNING PYTORCH WITH EXAMPLES\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa340e75",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf59681b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backword(input) \n",
    "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
    "        # 1/2(5x^3 - 3x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        # 一次求导\n",
    "        return grad_output * 1.5 * (5 * input ** 2 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1002e839",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "# >>> torch.device('cuda:0')\n",
    "# device(type='cuda', index=0) 第0个GPU\n",
    "device = torch.device(\"cuda:0\") # current cuda device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e300e823",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([-3.1416, -3.1384, -3.1353, -3.1322, -3.1290, -3.1259, -3.1227, -3.1196,\n         -3.1164, -3.1133], device='cuda:0'),\n tensor([ 8.7423e-08, -3.1430e-03, -6.2863e-03, -9.4292e-03, -1.2572e-02,\n         -1.5715e-02, -1.8858e-02, -2.2000e-02, -2.5143e-02, -2.8285e-02],\n        device='cuda:0'))"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create random to hold input and outputs\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "x[:10], y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80b31973",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(0., device='cuda:0', requires_grad=True),\n tensor(-1., device='cuda:0', requires_grad=True))"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create random Tensors for weights\n",
    "# torch.full 类似声明一个数组 tensor a = 0(cuda,require_grad=True)\n",
    "a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56bd9a52",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LegendrePolynomial3Backward' object has no attribute 'save_for_backword'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [6]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      5\u001B[0m P3 \u001B[38;5;241m=\u001B[39m LegendrePolynomial3\u001B[38;5;241m.\u001B[39mapply\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# 计算预测值\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m a \u001B[38;5;241m+\u001B[39m b \u001B[38;5;241m*\u001B[39m \u001B[43mP3\u001B[49m\u001B[43m(\u001B[49m\u001B[43mc\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43md\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# compute and print loss\u001B[39;00m\n\u001B[1;32m     11\u001B[0m loss \u001B[38;5;241m=\u001B[39m (y_pred \u001B[38;5;241m-\u001B[39m y)\u001B[38;5;241m.\u001B[39mpow(\u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39msum()\n",
      "Input \u001B[0;32mIn [2]\u001B[0m, in \u001B[0;36mLegendrePolynomial3.forward\u001B[0;34m(ctx, input)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(ctx, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m----> 4\u001B[0m     \u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_for_backword\u001B[49m(\u001B[38;5;28minput\u001B[39m) \n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m0.5\u001B[39m \u001B[38;5;241m*\u001B[39m (\u001B[38;5;241m5\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m3\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m3\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;28minput\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'LegendrePolynomial3Backward' object has no attribute 'save_for_backword'"
     ]
    }
   ],
   "source": [
    "# 设置超参数\n",
    "learning_rate = 5e-6\n",
    "for t in range(2000):\n",
    "    # use own function (Function.apply)\n",
    "    P3 = LegendrePolynomial3.apply\n",
    "    \n",
    "    # 计算预测值\n",
    "    y_pred = a + b * P3(c + d * x)\n",
    "    \n",
    "    # compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99: # 199，...\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    # autograd to backward\n",
    "    loss.backword()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad:\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: y = 0.0 + -1.0 * P3(0.0 + 0.30000001192092896 x)\n"
     ]
    }
   ],
   "source": [
    "print(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "x = torch.linspace(-math.pi, math.pi, 2000) # 从开始到结束的均匀分布\n",
    "y = torch.sin(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ -3.1416,   9.8696, -31.0063],\n        [ -3.1384,   9.8499, -30.9133],\n        [ -3.1353,   9.8301, -30.8205],\n        ...,\n        [  3.1353,   9.8301,  30.8205],\n        [  3.1384,   9.8499,  30.9133],\n        [  3.1416,   9.8696,  31.0063]])"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)   # x x^2 x^3\n",
    "xx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# MSE LOSS\n",
    "loss_fn = torch.nn.MSELoss(reduction=\"sum\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 8.817173957824707\n",
      "199 8.817168235778809\n",
      "299 8.817167282104492\n",
      "399 8.817168235778809\n",
      "499 8.817167282104492\n",
      "599 8.817167282104492\n",
      "699 8.817168235778809\n",
      "799 8.817167282104492\n",
      "899 8.817168235778809\n",
      "999 8.817168235778809\n",
      "1099 8.817168235778809\n",
      "1199 8.817168235778809\n",
      "1299 8.817168235778809\n",
      "1399 8.817168235778809\n",
      "1499 8.817168235778809\n",
      "1599 8.817167282104492\n",
      "1699 8.817168235778809\n",
      "1799 8.817167282104492\n",
      "1899 8.817167282104492\n",
      "1999 8.817167282104492\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients\n",
    "    # 不应该先更新权重后把梯度归0？\n",
    "    # model.zero_grad()\n",
    "    # 使用优化器\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # backward\n",
    "    loss.backward()\n",
    "\n",
    "    # update the weights\n",
    "    # with torch.no_grad():\n",
    "    #     for param in model.parameters():\n",
    "    #         param -= learning_rate * param.grad\n",
    "    # update parameters\n",
    "    optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "Linear(in_features=3, out_features=1, bias=True)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer = model[0]\n",
    "linear_layer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: y = 5.01128909036197e-07 + 0.8567413687705994 x + 5.009318329030066e-07 x^2 + -0.09332989156246185 x^3\n"
     ]
    }
   ],
   "source": [
    "# weights and bias\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "Sequential(\n  (0): Linear(in_features=3, out_features=1, bias=True)\n  (1): Flatten(start_dim=0, end_dim=1)\n)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "class Polynomial3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Instantiate four parameters\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 正态分布\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input: a Tensor\n",
    "        Return: a Tensor of output data\n",
    "        \"\"\"\n",
    "        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "\n",
    "    def string(self):\n",
    "        \"\"\"\n",
    "        define custom method on PyTorch modules\n",
    "        \"\"\"\n",
    "        return f'y = {self.a.item()} + {self.b.item()} * x + {self.c.item()} * x^2 + {self.d.item()} x^3'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result : y = -0.01847038045525551 + 0.8218784928321838 * x + 0.0031864470802247524 * x^2 + -0.08837152272462845 x^3\n"
     ]
    }
   ],
   "source": [
    "# 使用定义的模型\n",
    "model = Polynomial3()\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\n",
    "for t in range(2000):\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 预测 - 实际\n",
    "    loss = criterion(y_pred, y)\n",
    "\n",
    "    # Zero gradients perform a backward pass , and update the weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result : {model.string()}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999 1076.6103515625\n",
      "3999 578.549072265625\n",
      "5999 278.77447509765625\n",
      "7999 223.59188842773438\n",
      "9999 71.91000366210938\n",
      "11999 39.38667297363281\n",
      "13999 23.1971492767334\n",
      "15999 16.803993225097656\n",
      "17999 12.429099082946777\n",
      "19999 9.967989921569824\n",
      "21999 9.736001014709473\n",
      "23999 9.311260223388672\n",
      "25999 9.080161094665527\n",
      "27999 8.593727111816406\n",
      "29999 8.908875465393066\n",
      "Result: y = 0.0074686575680971146 + 0.8604275584220886 x + -0.0019180409144610167 x^2 + -0.09422893822193146 x^3 + 0.0001222019927809015 x^4 ? + 0.0001222019927809015 x^5 ?\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate five parameters and assign them as members.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "        self.e = torch.nn.Parameter(torch.randn(()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        For the forward pass of the model, we randomly choose either 4, 5\n",
    "        and reuse the e parameter to compute the contribution of these orders.\n",
    "\n",
    "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
    "        Python control-flow operators like loops or conditional statements when\n",
    "        defining the forward pass of the model.\n",
    "\n",
    "        Here we also see that it is perfectly safe to reuse the same parameter many\n",
    "        times when defining a computational graph.\n",
    "        \"\"\"\n",
    "        y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "        for exp in range(4, random.randint(4, 6)):\n",
    "            y = y + self.e * x ** exp\n",
    "        return y\n",
    "\n",
    "    def string(self):\n",
    "        \"\"\"\n",
    "        Just like any class in Python, you can also define custom method on PyTorch modules\n",
    "        \"\"\"\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'\n",
    "\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = DynamicNet()\n",
    "\n",
    "# Construct our loss function and an Optimizer. Training this strange model with\n",
    "# vanilla stochastic gradient descent is tough, so we use momentum\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)\n",
    "for t in range(30000):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 2000 == 1999:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result: {model.string()}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}